### use # to comment out the configure item

############ status ############
mode=train
#string: train/interactive_predict/test

############ Datasets(Input/Output) ############
datasets_fold=data
train_file=test
dev_file=test_100w.txt
#train_file=merge_abstract_1500w.txt
token_level=char
#token_level: word/char
checkpoint_dir=checkpoints
checkpoint_name=bert-base-chinese
vocabs_dir=data/vocabs
log_dir=data/logs
tfrecords_file_path=data/tfrecords_file

############ word2vec Configure ###########
stop_word_file=None
w2v_train_data=None
w2v_model_dir=model/word2vec_model
w2v_model_name=word2vec_model
w2v_model_dim=300
w2v_min_count=3
sg=cbow
# sg: cbow / skip-gram

############ Model Configure ###########
model=BERT
# BERT / TextCNN / TextRNN / TextRCNN / Transformer / FastText / HMCN
max_sequence_length=512
# bert 512
batch_size=32
max_to_keep=5
save_interval_updates=1000
learning_rate=0.00001
#BERT 1e-5 TextCNN: 5e-4
epoch=10
optimizer=Adam
bert_pretrain_path=bert-base-chinese
print_per_batch=10
is_early_stop=True
patient=3
dropout_rate=0.3
loss_type=focal
# focal/cross_entropy
# TextCNN model configure
num_filters=64
use_attention=True
attention_size=300
embedding_method=random
embedding_dim=512
# HMCN configure
hierarchical_depth=[384, 384, 384]
global2local=[16, 192, 512]

#TextRNN model configure
hidden_dim=2048
# Transformer 2048

#Transformer model configure
head_num=8
encoder_num=6

#HMCN model configure
hierar_penalty=0.5

measuring_metrics=[accuracy]
# string accuracy | precision | recall | f1


pb_model_sava_dir=saved_model